{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pyLDAvis.gensim_models\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "import re\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to scrap data from other websites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape reviews from a single Amazon URL\n",
    "def scrape_reviews(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
    "                      '(KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36'\n",
    "    }\n",
    "#deposit the data in an empty df\n",
    "    reviews_data = []\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find review containers (this is a very common structure)\n",
    "        reviews = soup.find_all('div', {'data-hook': 'review'})\n",
    "        #make a for loop to scrape each review\n",
    "        for review in reviews:\n",
    "            rating = review.find('i', {'data-hook': 'review-star-rating'})\n",
    "            text = review.find('span', {'data-hook': 'review-body'})\n",
    "            user = review.find('span', class_='a-profile-name')\n",
    "\n",
    "            reviews_data.append({\n",
    "                'Rating': rating.get_text(strip=True) if rating else 'N/A',\n",
    "                'Review Text': text.get_text(strip=True) if text else 'N/A',\n",
    "                'User': user.get_text(strip=True) if user else 'N/A',\n",
    "                'Source URL': url\n",
    "            })\n",
    "#make sure there is a response in case of error\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "    return reviews_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APP LOOKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Streamlit UI\n",
    "st.title(\"WebVoice: Know what your customers are saying quickly\")\n",
    "st.markdown(\"Enter one or more URLs below (one per line):\")\n",
    "\n",
    "input_text = st.text_area(\"Amazon Product URLs\", height=150)\n",
    "\n",
    "if st.button(\"Scrape Reviews\"):\n",
    "    urls = [line.strip() for line in input_text.splitlines() if line.strip()]\n",
    "    all_reviews = []\n",
    "\n",
    "    with st.spinner(\"Scraping reviews...\"):\n",
    "        for url in urls:\n",
    "            reviews = scrape_reviews(url)\n",
    "            all_reviews.extend(reviews)\n",
    "\n",
    "    if all_reviews:\n",
    "        df = pd.DataFrame(all_reviews)\n",
    "        st.success(f\"Scraped {len(all_reviews)} reviews from {len(urls)} links.\")\n",
    "        st.dataframe(df)\n",
    "        csv = df.to_csv(index=False)\n",
    "        st.download_button(\"Download as CSV\", csv, \"amazon_reviews.csv\", \"text/csv\")\n",
    "    else:\n",
    "        st.warning(\"No reviews found or scraping failed.\")\n",
    "        if st.warning (\"No reviews found or scraping failed.\"):\n",
    "            display st.link_button(\"Learn More\", url, *, help=None, type=\"secondary\", icon=None, disabled=False, use_container_width=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case there "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorscrape_learnmore = Path (LEARNMORESCRAPE.md)\n",
    "if st.warning (\"No reviews found or scraping failed.\"):\n",
    "    display st.button(\"Learn More\"):\n",
    "    # Read the content of the README.md file\n",
    "    readme_content = errorscrape_learnmore.read_text(encoding=\"utf-8\")\n",
    "    #display content\n",
    "    st.markdown(readme_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if st.warning (\"No reviews found or scraping failed.\"):\n",
    "    display st.link_button(\"Learn More\", url, *, help=None, type=\"secondary\", icon=None, disabled=False, use_container_width=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st.sidebar.markdown(\"#Topic Model\")\n",
    "st.sidebar.markdown(\"#Sentiment Analysis\")\n",
    "st.sidebar.markdown(\"#Both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = st.text_area(\"insert custom stopwords:\", height = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streamlit App UI\n",
    "\n",
    "if uploaded_file:\n",
    "    reviews = pd.read_csv(all_reviews)\n",
    "\n",
    "    # Preprocessing\n",
    "\n",
    "    st.subheader(\"1. Preprocessing Text\")\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    custom_stopwords = custom_stopwords\n",
    "\n",
    "    def preprocess(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r\"[^a-z\\s]\", \"\", text)\n",
    "        tokens = text.split()\n",
    "        tokens = [stemmer.stem(word) for word in tokens if word not in stop_words and word not in custom_stopwords]\n",
    "        return tokens\n",
    "    reviews['tokens'] = reviews['review_text'].fillna(\"\").apply(preprocess)\n",
    "\n",
    "    dictionary = corpora.Dictionary(reviews['tokens'])\n",
    "    corpus = [dictionary.doc2bow(text) for text in reviews['tokens']]\n",
    "\n",
    "  \n",
    "    # LDA Topic Modeling\n",
    " \n",
    "    st.subheader(\"2. Training LDA Model\")\n",
    "    num_topics = st.slider(\"Select number of topics\", min_value=5, max_value=60, value=10, step=5)\n",
    "\n",
    "    lda_model = gensim.models.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        random_state=1,\n",
    "        passes=10,\n",
    "        alpha='auto',\n",
    "        per_word_topics=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # Display Topics\n",
    "\n",
    "    st.subheader(\"3. Extracted Topics\")\n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        st.write(f\"**Topic {idx}:** {topic}\")\n",
    "\n",
    "\n",
    "    # Topic Visualization\n",
    "\n",
    "    st.subheader(\"4. Interactive Topic Visualization\")\n",
    "    with st.spinner(\"Generating visualization...\"):\n",
    "        vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "        pyLDAvis.save_html(vis, 'lda_vis.html')\n",
    "        st.components.v1.html(open('lda_vis.html', 'r', encoding='utf-8').read(), height=800)\n",
    "\n",
    "\n",
    "    # Topic and Rating Correlation\n",
    "\n",
    "    st.subheader(\"5. Topic Prevalence by Rating\")\n",
    "\n",
    "    topic_dist = []\n",
    "    for row in corpus:\n",
    "        topic_probs = lda_model.get_document_topics(row, minimum_probability=0)\n",
    "        topic_array = np.array([prob for _, prob in topic_probs])\n",
    "        topic_dist.append(topic_array)\n",
    "\n",
    "    topic_dist_df = pd.DataFrame(topic_dist, columns=[f\"Topic {i}\" for i in range(num_topics)])\n",
    "    topic_dist_df['Rating'] = reviews['review_star']\n",
    "\n",
    "    mean_topic_by_rating = topic_dist_df.groupby(\"Rating\").mean()\n",
    "    st.line_chart(mean_topic_by_rating)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = st.text_area(\"insert custom stopwords:\", height = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom transformer-based sentiment analyzer function\n",
    "def transformer_sentiment_analyzer(review):\n",
    "    # Get the sentiment from the pipeline; truncate text if necessary\n",
    "    pipeline_return = sentiment_pipeline(review, truncation=True)\n",
    "\n",
    "    # Return \"negative\" if the label is \"NEGATIVE\", otherwise \"positive\"\n",
    "    label = pipeline_return[0]['label']\n",
    "    if label == \"NEGATIVE\":\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"positive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will download the model the first time it runs\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    return pipeline(\"sentiment-analysis\")\n",
    "\n",
    "sentiment_analyzer = load_model()\n",
    "\n",
    "\n",
    "# Button to trigger sentiment analysis\n",
    "if st.button(\"Analyze Sentiment\"):\n",
    "    if all_reviews.strip():\n",
    "        # Run sentiment analysis on the input\n",
    "        result = sentiment_analyzer(all_reviews)[0]\n",
    "        \n",
    "        # Display the result\n",
    "        st.success(f\"**Sentiment:** {result['label']} \\n\\n**Confidence:** {result['score']:.2%}\")\n",
    "    else:\n",
    "        st.warning(\"Please enter some text before clicking the button.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
